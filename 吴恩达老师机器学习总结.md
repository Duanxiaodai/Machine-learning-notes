## 第一章--机器学习介绍

第一章：课程中吴恩达老师主要讲解了什么是机器学习，并且机器学习分为监督学习和无监督学习，并且简单的介绍了一下，监督学习有回归和分类，无监督学习有聚类等。

## 第二章--单变量线性回归算法

第二章：吴恩达老师讲解单变量的线性回归，他从代价函数开始讲，为了找到最好的代价函数，也就是模型，我们需要找到损失函数（代价函数）的最小点，要想找到损失函数的最小点我们需要使用梯度下降的算法来找到最小的那个点。

## 第三章--线性代数回顾

第三章：吴恩达老师讲解了机器学习中的常用线代的知识。

## 第四章--多元线性回归

第四章：第一节课讲解多维特征，我们前面学习的线性回归是单特征的，就是只有一个特征x，而实际问题中都是多维特征，多维特征就是指有一个样本有多个特征，要想表示一个样本，那么我们可以使用第三章学习到的向量（有下标的都不是向量，没有下标的是向量，表示一个整体）来表示：


同理我们的参数θ也不再是一维了，而是


所以最终我们的假设函数可以表示为：


我们现在可以理解为多维特征就是多个特征，并且我们可以使用向量来表示X和θ,进而表示hθ(x),我们现在已经有了多维特征的假设函数了，我们如何找到最好的参数θ呢？

这就进入了本章课程的第二节课----多变量的梯度下降，和单变量的差不多，就是更新的θj多了。

然后第三、四节课分别为--梯度下降的使用技巧-特征缩放和梯度下降法实践-学习率。因为特征和特征之间有可能数值差距比较大，为了让每个特征都能公平的发挥作用，这里我们使用特征缩放，特征缩放一般：


下面还有一个问题是学习率，因为学习率太小，则梯度下降的步伐太慢了，要是太大的话有可能错过代价函数最优点，所以选择一个好的学习率很重要。

第五节是特征和多项式回归。我们先来看一下什么是线性回归：

线性回归：线性就是每个变量的指数都是1，它的形态是直线形态或者是超平面形态

非线性回归：非线性回归就是至少有一个变量的指数不是1（二次或者是多次），它的形态是曲线形态。

所以我们可以说我们前面的模型都是线性回归的，而本节课程中我们会知道两点：一个好的特征选择对应一个好的模型，还有一个就是非线性的回归如何才能转变成线性回归，从而使用线性回归的梯度下降来完成找到非线性回归的最优解的目的。

第六节课是特征工程：可以理解为特征工程是和梯度下降相对应的，现在我们要想求θ我们不只有梯度下降，我们还有另外一种方式了，这种方式就是特征工程：

它是直接求导，令导数=0，从而直接找出最优点，我们要想使用特征工程，我们要将我们的样本构建一个设计矩阵X


这样我们求解θ就可以直接：


我们要将特征工程和我们前面的多维特征区别，特征工程是将整个训练集用一个矩阵表示，而我们多维特征中我们用一个向量表示一个样本

第七节课是针对第六节课来说的，因为特征工程公式中有可逆条件，要是不可逆还可以使用特征工程吗？以及什么时候使用梯度下降好，什么时候使用正规方程好？本节课程会告诉你。

## 第五章

第五章本系列并没有进行总结，原因就是这里吴恩达老师讲解了octave语言的一些编程的知识，考虑到现在使用python完成机器学习比较常见，而且python也比较火，所以并没有总结第五章的内容。

## 第六章--逻辑回归

第六章，我们前面学习了回归的问题，本章节学习分类的问题，我们学习的是逻辑回归算法，一开始吴恩达老师就是告诉大家为什么分类问题不能使用线性回归来完成，从而引出了逻辑回归算法。之后给出了逻辑回归算法的假设表示，并且告知了我们这个模型表示的就是y=1的概率。为了让大家更加清楚的明白逻辑回归的假设函数是什么意思，这里吴恩达老师引进了判定边界的这个概念，判定边界就是将正负样本分离开的边界。我们现在已经有了假设函数了，所以我们要使用最小化代价函数的方式来求出最好的θ，从而得到我们的逻辑回归的模型。但是吴恩达老师并不是直接的引入了逻辑回归的代价函数，而是将线性回归的代价函数引入，发现此时的代价函数是一个非凸函数，所以用线性回归平方的方式创建的代价函数是不行的，从而引出了逻辑回归的代价函数。因为引入代价函数的时候，是分成两个类别的，一个类别是y=0，一个类别是y=1，此时我们为了使用方便，我们将其合并为一个式子：


此时逻辑回归的代价函数有了，那么下面我们就可以通过梯度下降算法来实现最小化代价函数了


我们可以发现这个逻辑回归的梯度下降和线性回归的梯度下降其实形式一样，但是二者是由很大区别的，根本原因就是hθ(x)二者是不一样的。

下面又讲了一个高效优化算法，我们知道实现求J(θ)的最小值，不只有梯度下降这一种方法，还是有其它方法的，这里讲解了一些高级的优化算法，它们能比梯度下降算法更快，而且讲解了具体的用法，我们需要认为定义一个costfunction方法。

我们上面讲解的分类的问题都是二分类的问题，所以最后讲解了如何利用二分类问题来完成多分类的问题。

## 第七章--正则化

第7章主要讲解的是正则化，为什么需要正则化呢？因为我们为了拟合比较复杂的数据的时候，往往需要多项式（x1x2，x1²，x2²）来拟合，但是任何东西都有一个限度，当多项式过多之后就会造成过拟合的现象，我们的正则化技术就可以将一些意义不大的多项式的系数θ弄成足够小甚至为0，这样多项式就不会过多的，就可以解决过拟合的问题了。

正则化是什么呢？

其实就是在损失函数上加上一个正则化项，就可以了。

然后下面的两个课程就是分别从线性回归算法和逻辑回归算法来讲解的正则化的具体实现。

## 第八章--神经网络

第8章和第9章都在讲解神经网络

8-1为什么用神经网络呢？神经网络可以很好的解决特征n很大和非线性分类的问题。

8-2这节意义不大，省略

8-3和8-4两节课讲解了前向传播的过程，神经网络进行前向传播其实可以理解为将初始输入的特征x经过每一层的权重和非线性sigmoid函数的作用不断进行前向传播到最后一层隐藏层得到的a，此时的a和输入层输入的x相比是更加厉害了（可能比x²和x³更加厉害），就是说这个特征a相比初始的特征x更加完美，最后相当于进行了一次很牛的逻辑回归，因为这次逻辑回归的特征是经过隐藏层升级的特征，这就是神经网络厉害的原因。


8-5和8-6特征的直观理解：两节课就是想告诉我们要想实现一个最终的功能，隐藏的功能必不可少，其实可以理解为隐藏层是在不断升级我们的特征，最终我们用这个升级的特征来完成我们最终的任务。

8-7多分类，我们学过逻辑回归的多分类，那么此时的样本标签y和神经网络的输出hθ(x)应该如何表示呢？

## 第九章--神经网络的学习

9-1神经网络的代价函数的作用是什么？

9-2代价函数最小的时候，神经网络的参数θ才是最好的，这样进行前向传播我们才能得到最优的特征用于最终的逻辑回归，那么此时的任务就是想要让代价函数最小，代价函数最小的方法就是梯度下降，而梯度下降所需要每个参数的偏导数，这个偏导数可以使用反向传播算法来求出来。

9-4实践中如何在octave中将我们的参数矩阵展开成长向量，然后怎么将长向量又转成矩阵。

9-5如何验证反向传播算法是正确的？使用梯度检验方法，验证我们求得的偏导数是正确的

9-6我们的参数θ是如何进行初始化呢？随机初始化

9-6所有东西揉在一起讲解一下

## 第十章--应用机器学习的建议

这章节吴恩达老师并没有讲解具体的机器学习的算法，而是从应用角度来给出我们的机器学习算法的应用建议。

本章节开始的时候就是讲解我们有些时候算法效果并不好，但是我们应该怎么办呢？方法很多，比如增加特征数，增加样本数，但是有些问题只能对症下药，如果没有对症下药，那么真是在白费力气，这里吴恩达引出了机器学习的诊断方法，能够判断我们算法出了什么问题，从而对症下药。

如何评价我们当前假设是不是一个好的假设呢？我们可以将我们的数据分成两部分，一部分是训练集，另外一部分是测试集，我们使用训练集训练出来的假设（最小化J(θ)），然后使用测试集来测试，如果测试效果好的话，那么这个假设就是一个好的假设，如果测试的效果不好的话，那么这个假设就不是一个好的假设。

我们已经知道了怎么来确定这个假设是好还是坏，这个用处很大，我们可以用来判断模型假设选择最合适的多项式的次数是几次？因为多了一个多项式的次数d，所以我们要将我们的数据集分为训练集（拟合θ）、交叉验证集（拟合d）、测试集来分别处理，找到交叉验证集中效果最好的。我们也可以将训练集和交叉验证集的代价函数误差关于多项式的次数画在一起可以帮助我们判断什么时候的我们的模型是处于欠拟合的状态，什么时候处于过拟合的状态，从而选出最好的那个d。

正则化参数λ又和高偏差和高方差什么关系呢？我们可以画出某个模型的训练集和交叉验证集模型的代价函数关于λ误差，以此来确定什么该模型什么时候λ为过拟合，什么时候为欠拟合，什么时候最好的λ。

我们会画出两个图，一个是根据多项式次数d的，一个是关于λ的，它们可以帮助我们判断最好的d和λ。还有一种图像叫做学习曲线：

学习曲线是训练集和交叉验证集模型的代价函数关于样本m误差，通过这个我们可以判断这个模型此时是处于高偏差还是处于高方差问题，并根据这个问题确定具体的解决方案，对症下药。


后面讲解了一些关于神经网络的过拟合和欠拟合的一些问题和解决方式

## 第十一章--机器学习系统设计

本章节也没有讲解具体的算法，而是讲解了机器学习系统设计。我们构建系统的时候，我们有很多方法可以选择，但是我们究竟选择哪些方法来优化我们的算法呢？

两个角度：

角度一：一般先建立一个不算太复杂的算法然后实现它，然后我们使用交叉验证集来验证它找出我们算法是处于高偏差还是高方差问题，并进行相应的优化，增加特征还是增加样本等等。

角度二：然后我们可以对这个算法进行误差分析，我们可以通过误差分析来判断我们的算法有什么问题，然是再针对问题来找解决问题的方法。

有些时候我们有一些处理问题的方法，那么这个方法用还是不用呢？或者说用哪种方法比较好呢？我们可以使用误差评估度量的方式，它是一个具体的数值，它会直观的告诉我们，哪个好，哪个不好。这种误差度量就是叫做错误率（交叉验证集上的）。

有些时候使用错误率这种误差度量有一种问题是解决不了的，那就是偏斜类的问题，所以这里我们引出了解决偏斜类问题的度量，查准率和召回率，它们能够帮助我们解决偏斜类的度量问题。

我们在很多应用中，都是希望能够保证查准率和召回率的相对平衡。具体来说是希望查准率高一些还是希望召回率高一些，这个需要根据实际来判断。因为临界值的不同，查准率和召回率是不同的。

我们现在有多个算法（同样的算法，临界值不同），我们如何决定那个算法最好呢？

一种方法是计算 F1 值（F1 Score），其计算公式为：


最后吴恩达讲解了一个好的算法应该是多特征可以有效的降低偏差，大量的数据集可以有效的降低方差，这两个结合在一起就是一个好的算法。就是说建立一个低偏差的算法，然后喂给它大量的数据。

## 第十二章--支持向量机

这一章终于要讲解一个算法了，这个算法就是支持向量机，一开始吴恩达老师就从逻辑回归开始切入引入了支持向量机的损失函数以及支持向量机的假设是什么？

支持向量机究竟干了一件什么事情呢？其实它就是将正负样本分开的大间距的分类器，这样分类效果会很好。

之后吴恩达老师从数学角度讲解了为什么支持向量机会是大间距的分类器。

支持向量机是可以解决线性可分的，如果线性不可分的话，那么支持向量机无法完成的，那么此时就需要引入了核函数这个概念，将数据映射到高维空间，这样就线性可分了。

之后讲解了支持向量机怎样使用，以及什么时候使用逻辑回归、什么使用支持向量机、什么时候使用神经网络。

## 第十三章--聚类

第一节课中，吴恩达老师讲解了什么是非监督学习，以及在解决什么问题的时候可以使用非监督学习。

第二节课中，本节课讲解了常用的非监督学习算法K-means算法的原理。

第三节课中，本节课讲解了K-means算法的代价函数

第四节课中，k均值算法需要随机初始化聚簇中心，聚簇中心的不同，聚类效果也可能不同，此时可能陷入局部最优，所以为了解决这个问题我们需要运行多次K-means来找到最好的聚类效果。

第五节课中，我们使用K-means将我们的数据聚成多少类呢？我们可以使用可视化的方式，或者采用试的方式，或者根据实际需要，如果符合肘部规则，那么可以使用肘部规则来判断聚成多少类。

## 第十四章--降维

第一节课中，大概讲解一下降维干了一件什么事，降维到k维，那么就要找到k维向量构成的投影面，然后所有样本对这个投影面进行投影，那么我们就得到降维的目的了，得到降维之后的特征z1，z2..zk。

第二节课中，如何使用降维算法来完成数据的可视化，将数据降维到二维或者三维。

第三节课中，更一般的是如果我们有n维的数据想要降到k维，那么这种情况下，我们需要找到k个方向来对数据进行投影，从而最小化投影误差。

第四节课中，假如我们要将数据降维到k维，那么k维的方向向量如何找到？找到之后，映射的之后如何计算新的特征向量z。

第五节课，具体降维降到多少维度最合适呢？本节课就是解决这个问题。

第六节课，我们从高维降维到低维，那么如何从低维回到原始的特征呢？本节课解决这个问题。

第七节课，主成分分析算法的应用。

## 第十五章--异常检测

异常检测算法是一个非监督学习算法，但是它为了验证异常检测的正确与否，常常和标签来结合使用，所以它又有一些监督学习的影子。

因为有一些问题中样本偏斜类，所以我们可以使用异常检测算法来完成类别的判断，这也是我们为什么在这种情况下不使用监督学习算法的原因。

异常检测算法的核心就是通过样本集数据拟合概率分布p(x),之后我们就可以通过p(x)来判断我们的新样本是否异常了。要想拟合出p(x)我们需要使用高斯分布来完成。

开发完成一个异常检测算法之后，我们应该这个评价异常检测系统的好坏，因为是偏斜类问题，我们需要使用F1值来评价

我们要想开发一个异常检测算法最重要的就是特征的问题，也就是让什么特征输入到算法中，我们可以使用误差分析的方式来确定，如果我们的特征不符合高斯分布，我们可以随其进行调整(log).

我们前面学习的高斯分布有些时候并不能认出异常，为了解决这种问题，我们可以将特征进行组合或者使用多元高斯核函数来解决这个问题。

## 第十六章--推荐系统

构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。

每个电影样本由两个特征x1和x2构成，用户给每部电影的评分是y，每个用户有属于自己的模型参数θ，每个用户进行独立的线性回归。

协同过滤，它能实现对特征的学习，自动学习到算法所需要使用的特征。

学习特征x：已知参数θ和每个人的已评分

学习参数θ：已知特征x和每个人的已评分

具体做法是这样的：随机初始化θ->x->θ->x…最终算法将收敛到一组合理的电影特征以及一组对不同用户参数的估计，这就是基本的协同过滤的算法（这实际上并不是我们将要使用的算法）。具体来说此时我们的代价函数变为了：


协同过滤算法的向量化的形式叫做低矩阵分解

均值化归一化就是为了解决一个没有给任何电影评分的用户的预测它对每部电影评分的解决方案。

## 第十七章--大规模机器学习

一个低偏差的学习算法，然后使用大量的数据来训练，问题是当我们使用大数据进行梯度下降的时候计算量会非常的大，此时我们可以不使用批量梯度下降而是使用随机梯度下降。

还有一种算法叫做小批量梯度下降

我们前面学习过如何判断梯度下降算法是正常的，那么随机梯度下降算法如何判断它是在收敛呢？

当我们有源源不断地用户流并且产生源源不断地数据流的时候，我们没有必要存储这些数据，我们可以使用在线学习算法，从而不断调整我们的模型，从而适应用户的偏好。

有些机器学习问题因为数据集太大以至于不能够在一台计算机上运行，这次我们讲解的大规模的机器学习称为映射约减。

## 第十八章--实例图片文字识别

首先吴恩达老师讲解了照片OCR算法的流水线，他每一部分都是由一个模块组成的。然后讲解了它具体是怎么工作的，也就是这个流水线是如何识别出图片文字的。

如果我们的算法需要大量的数据，那么我们如何才能创造出大量的数据呢？这里吴恩达老师还是以OCR流水线为例，讲解如何才能人工创造出图片数据呢？

最后吴恩达老师讲解了上限分析，从而帮助我们确定我们的流水线哪个模块具有最大的潜力能够帮助我们提高整体系统的效果
